{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Life simulation with Spark and GraphFrames "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настройка на Spark\n",
    "\n",
    "это можно делать множеством разных способов - например, так."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mk/local/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')\n",
    "sys.path.append('/home/mk/local/spark-2.4.7-bin-hadoop2.7/python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключаем Graphframes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages graphframes:graphframes:0.8.1-spark2.4-s_2.11 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"life_simu\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir('graphframes_cps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n",
    "from graphframes.lib import AggregateMessages as AM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим немного функций (борьба со сложностью)...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creDeltaDf():\n",
    "    \"\"\" создает специальный датафрейм для ребер и соседей \"\"\"\n",
    "    \n",
    "    return spark.createDataFrame(\n",
    "        [\n",
    "            [ -1, -1 ],\n",
    "            [ -1, 0 ],\n",
    "            [ -1, 1 ],\n",
    "            [ 0, 1 ],\n",
    "            [ 1, 1 ],\n",
    "            [ 1, 0 ],\n",
    "            [ 1, -1 ],\n",
    "            [ 0, -1 ],\n",
    "        ],\n",
    "        [\"dx\", \"dy\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creSurrDF(df):\n",
    "    \"\"\" возвращает датафрейм с колонками nv,nx,ny, содержащими координаты окружающих исходный \n",
    "    датафрейм точками\n",
    "    \"\"\"\n",
    "\n",
    "    delt = creDeltaDf()\n",
    "    \n",
    "    res = (\n",
    "        df.crossJoin(delt) # добавляем для каждой точки ее окружение\n",
    "        .selectExpr(\"0 as nv\", \"x+dx as nx\", \"y+dy as ny\") # вычисляем координаты точек окружения\n",
    "        .distinct() # убираем дубликаты\n",
    "        .join(      # убираем новые точки, которые попали на существующие точки\n",
    "            df,(df[\"x\"]==f.col(\"nx\")) & (df[\"y\"]==f.col(\"ny\")),\"left_anti\"\n",
    "        )\n",
    "    )\n",
    "    return df.union(res) # конкатенируем исходный и окружающий датафреймы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creEdges(df):\n",
    "    \"\"\" создает датафрейм с ребрами для графа \"окружаюещего датафрейма\" \n",
    "    ребра соединяют каждую ячейку с 8 соседями (граничные - только с существующими)\n",
    "    \"\"\"\n",
    "    \n",
    "    # выражения, определяющие - попала ли новая вершина в наш граф\n",
    "    rx = ((f.col(\"x\")+f.col(\"dx\"))<=f.col(\"maxx\"))&(f.col(\"minx\")<=(f.col(\"x\")+f.col(\"dx\")))\n",
    "    ry = ((f.col(\"y\")+f.col(\"dy\"))<=f.col(\"maxy\"))&(f.col(\"miny\")<=(f.col(\"y\")+f.col(\"dy\")))\n",
    "    \n",
    "    # получение id для dst вершины\n",
    "    dstCol = f.concat_ws(\":\",(f.col(\"x\")+f.col(\"dx\")).cast(\"string\"),(f.col(\"y\")+f.col(\"dy\")).cast(\"string\"))\n",
    "    \n",
    "    # добавим к каждой строке колонки макс и мин координат\n",
    "    dfn = df.crossJoin(df.selectExpr(\"max(x) as maxx\",\"min(x) as minx\",\"max(y) as maxy\",\"min(y) as miny\"))\n",
    "\n",
    "    # создаем дуги - формируем id вершин как строку \"x:y\"\n",
    "    delt = creDeltaDf()\n",
    "    eDf = (\n",
    "        dfn.crossJoin(delt)\n",
    "        .withColumn(\"src\",f.concat_ws(\":\",f.col(\"x\").cast(\"string\"),f.col(\"y\").cast(\"string\")))\n",
    "        .withColumn(\"dst\",f.when(rx & ry,dstCol))\n",
    "        .filter(\"dst is not NULL\")\n",
    "        .select(\"src\",\"dst\")\n",
    "    )\n",
    "    return eDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creVertices(df):\n",
    "    \"\"\" создает вершины по датафрейму с координатами точек \n",
    "    вершина получает id = строке \"x:y\"\n",
    "    \"\"\"\n",
    "    \n",
    "    vDf = (\n",
    "        df\n",
    "        .withColumn(\"id\",f.concat_ws(\":\",f.col(\"x\").cast(\"string\"),f.col(\"y\").cast(\"string\")))\n",
    "        .select(\"id\",\"v\")\n",
    "    )\n",
    "    return vDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendMsg(gr):\n",
    "    \"\"\" рассылает сообщения от живых клеток, возвращает датафрейм (id,totn) \"\"\"\n",
    "    res = (\n",
    "        gr.aggregateMessages(\n",
    "            f.sum(\"msg\").alias(\"totn\"),\n",
    "            None,\n",
    "            f.when(AM.src[\"v\"]==1,f.lit(1)).otherwise(f.lit(0)) \n",
    "        )\n",
    "    )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creNewDF(df,gr):\n",
    "    \"\"\" создает новый датафрейм с выжившими ячейками \"\"\"\n",
    "\n",
    "    # правила \"выживания\" для заполненной и пустой ячеек\n",
    "    eRule = ((f.col(\"totn\")==2)|(f.col(\"totn\")==3))\n",
    "    nRule = (f.col(\"totn\")==3)\n",
    "    \n",
    "    # формирование новой ячейки по правилам выживания\n",
    "    newCell = f.when(f.col(\"v\")==1,f.when(eRule,f.lit(1)).otherwise(f.lit(0))).otherwise(f.when(nRule,f.lit(1)).otherwise(f.lit(0)))\n",
    "    \n",
    "    res = (\n",
    "        df\n",
    "        .join(gr.vertices,\"id\")\n",
    "        .withColumn(\"nv\", newCell)\n",
    "        .filter(\"nv=1\")\n",
    "        .select(\"id\",\"nv\")\n",
    "        .withColumn(\"x\",f.split(f.col(\"id\"),\":\").getItem(0).cast(\"int\"))\n",
    "        .withColumn(\"y\",f.split(f.col(\"id\"),\":\").getItem(1).cast(\"int\"))\n",
    "        .select(\"nv\",\"x\",\"y\")\n",
    "        .withColumnRenamed(\"nv\",\"v\")\n",
    "    )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printField(df):\n",
    "    \"\"\" печатает поле, заданное в датафрейме \"\"\"\n",
    "    \n",
    "    # коллектим диапазоны и координаты точек\n",
    "    maxx,minx,maxy,miny = df.selectExpr(\"max(x)\",\"min(x)\",\"max(y)\",\"min(y)\").collect()[0]\n",
    "    res = df.collect()\n",
    "\n",
    "    # создаем пустое поле\n",
    "    rows = []\n",
    "    for i in range(maxy-miny+1):\n",
    "        rows.append([None]*(maxx-minx+1))\n",
    "        \n",
    "    # заполняем точками (пустая=*, заполненная=-)\n",
    "    for r in res:\n",
    "        rows[r[2]-miny][r[1]-minx] = \"*\" if r[0]==1 else \"-\"\n",
    "\n",
    "    # печатаем, добавляя отсутствующие в датафрейме ячейки в виде .\n",
    "    for r in rows:\n",
    "        pLine = []\n",
    "        for el in r:\n",
    "            if el is None:\n",
    "                pLine.append(\".\")\n",
    "            else:\n",
    "                pLine.append(el)\n",
    "        print(\"\".join(pLine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начальные данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несколько примеров начальных колоний клеток (для отладки и вообще)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пульсар с самого начала\n",
    "dfo = spark.createDataFrame(\n",
    "    [\n",
    "        [ 1, 0, 0 ],\n",
    "        [ 1, 0, 1 ],\n",
    "        [ 1, 0, 2 ],\n",
    "    ],\n",
    "    [\"v\",\"x\",\"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пульсар через небольшое количесто итераций\n",
    "dfo = spark.createDataFrame(\n",
    "    [\n",
    "        [ 1, 1, 2 ],\n",
    "        [ 1, 2, 3 ],\n",
    "        [ 1, 2, 2 ],\n",
    "        [ 1, 2, 1 ],\n",
    "        [ 1, 3, 2 ],\n",
    "    ],\n",
    "    [\"v\",\"x\",\"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Код: его исполняем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итерация \"жизни\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo.write.format(\"orc\").mode(\"overwrite\").save(\"curdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ячейку ниже нужно выполнять - каждое выполнение = очередная итерация \"жизни\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..---..\n",
      ".--*--.\n",
      "--*-*--\n",
      "-*---*-\n",
      "--*-*--\n",
      ".--*--.\n",
      "..---..\n"
     ]
    }
   ],
   "source": [
    "cDf = spark.read.format(\"orc\").load(\"curdf\")\n",
    "surDf = creSurrDF(cDf) # окружаем колонию пустыми клетками\n",
    "surDf.cache()\n",
    "printField(surDf)\n",
    "\n",
    "# создаем граф (т.е. ребра и вершины)  включая пустые клетки\n",
    "eDf = creEdges(surDf)\n",
    "vDf = creVertices(surDf)\n",
    "nGr = GraphFrame(vDf,eDf)\n",
    "\n",
    "resDf = sendMsg(nGr) # рассылаем сообщения из \"живых\" клеток\n",
    "nDf = creNewDF(resDf,nGr) # получаем итоговый датафрейм для следующей итерации\n",
    "nDf.write.format(\"orc\").mode(\"overwrite\").save(\"curdf\")\n",
    "\n",
    "none = surDf.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
