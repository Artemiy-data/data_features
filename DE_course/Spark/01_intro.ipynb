{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что такое Spark\n",
    "\n",
    "`Apache Spark is a unified computing engine and a set of libraries for parallel data processing on\n", 
    "computer clusters.`\n",
    "\n",
    "* способ параллельного программирования (как Map Reduce)\n",
    "* набор библиотек (Scala) и выполняемых файлов (spark-submit)\n",
    "* гораздо более функциональный, чем Map Reduce\n",
    "* похож на pandas\n",
    "* активно развивается (только что вышле версия 3.0)\n",
    "* мы будем использовать версию 2.4\n",
    "* нет привязки к Hadoop, но мы будем рассматривать spark именно в связке с Hadoop\n",
    "\n",
    "Хорошая книга от автора Spark - https://github.com/databricks/Spark-The-Definitive-Guide\n",
    "\n",
    "Активно ей пользуюсь сам, большинство материала взято оттуда."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что входит в Spark\n",
    "\n",
    "![](spark_toolkit.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что такое pyspark\n",
    "\n",
    "* spark предоставляет API на нескольких языках\n",
    "* наиболее популярное python API - pyspark\n",
    "    * Scala, Java, SQL, R\n",
    "* производительность не страдает (разберем ниже)\n",
    "\n",
    "`If you use just the Structured APIs, you can expect all languages to have similar performance characteristics`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как работают Spark приложения\n",
    "\n",
    "![](spark_apps.png)\n",
    "\n",
    "* `driver`: исполняет программу\n",
    "    * поддерживает инфорацию о приложении\n",
    "    * исполняет код программы \n",
    "    * планирует и запускает работу executor-ов\n",
    "* `executor`: выполняет вычисления\n",
    "    * исполняет код, переданный driver-ом (JAR)\n",
    "    * отчитывается перед driver-ом о состоянии процесса вычисления\n",
    "* `cluster manager`: управляет физическими машинами и выделяет ресурсы spark приложениям\n",
    "    * standalone, YARN, Mesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запуск приложения и режимы исполнения \n",
    "\n",
    "* приложения могут быть\n",
    "    * интерактивными (shell) - python, scala, SQL\n",
    "    * скомпилированными (JAR)\n",
    "* `execution mode`: возможность управления размещением процессов (driver, executors)\n",
    "    * `cluster mode`: все в кластере \n",
    "    * `client mode`: driver работает вне кластера, executor-ы - в кластере\n",
    "    * `local mode`: все работает локально (потоки)\n",
    "* `spark session`: объект, через который происходит работа со spark-ом (driver-ом) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основные абстракции\n",
    "\n",
    "![](rdd_dataframe.png)\n",
    "\n",
    "* `RDD (Resilient Distributed Dataset)`: набор объектов, разбитых на разделы (`partitions`)\n",
    "* `Dataset`: набор типизированных записей, разбитых на разделы  \n",
    "* `Dataframe`: набор записей типа `Row`, разбитых на разделы\n",
    "\n",
    "* `Low level API` (RDD)\n",
    "* `Structured API` (Dataset, Dataframe)\n",
    "* Dataset: только в Scala и Java API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe \n",
    "\n",
    "![](dataframe.png)\n",
    "\n",
    "* таблица, состоящая из строк и столбцов\n",
    "    * похожа на датафреймы в pandas и R\n",
    "* `schema`: список, определяющий структуру строки (имена и типы колонок)\n",
    "* `partition`: набор строк датафрейма, расположенных на одном узле кластера\n",
    "    * определяют возможность параллелизма (executors + partitions)\n",
    "* `Row`: строка датафрейма (объект)\n",
    "* `Column`: элемент строки (объект)\n",
    "* spark работает со своими типами данных (ByteType, IntegerType, StringType, ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трансформации и действия\n",
    "\n",
    "* методы spark делятся на трансформации (`transformation`) и действия (`action`)\n",
    "* все объекты (Dataframe, RDD, ...) неизменяемы (immutable)\n",
    "* чтобы изменить объект нужно задать инструкцию его изменения (transformation)\n",
    "* результат трансформации - объект (например, Dataframe)\n",
    "* `lazy evaluation`: трансформации выполняются только во время действия (`action`)\n",
    "    * используется оптимизатор (`Catalyst`)\n",
    "    * план исполнения можно посмотреть\n",
    "* `action`: вычисление, возвращающее результат\n",
    "    * подразумевает перемещение данных между экзекьюторами и драйвером\n",
    "    * посмотреть данные на консоли\n",
    "    * собрать данные на драйвере (с преобразованием в `native` тип данных)\n",
    "    * сохранение данных в источник"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Движение данных\n",
    "\n",
    "![](ideal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основные действия\n",
    "\n",
    "(для материалов, не слайд)\n",
    "\n",
    "Основными действиями (`action`), которыми мы будем далее активно пользоваться, являются:\n",
    "\n",
    "**collect()**\n",
    "\n",
    "Возвращает dataframe как список строк (list of Row)\n",
    "\n",
    "**take()**\n",
    "\n",
    "Аналогично collect(), только первые N строк\n",
    "\n",
    "**first()**\n",
    "\n",
    "Возвращает первую строку, как Row\n",
    "\n",
    "**count()**\n",
    "\n",
    "Возвращает количество строк в датафрейме\n",
    "\n",
    "\n",
    "**saveAs<file>()**\n",
    "\n",
    "Сохраняет датафрейм в файл (в HDFS).\n",
    "\n",
    "**saveAsTable()**\n",
    "\n",
    "Сохраняет датафрейм в таблицу (Hive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
